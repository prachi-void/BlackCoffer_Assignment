{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17b4fc9c",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <h1>Blackcoffer</h1>\n",
    "    <h4>Submitted by: Prachi Nikalje</h4>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1e971024",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7e3db452",
   "metadata": {},
   "source": [
    "<html><body><h2>1. Setup and Initialization</h2>\n",
    "    <p>Begin by importing necessary libraries and initializing any variables or settings needed for the conversion process.</p>\n",
    "\n",
    "    <h2>2. Data Retrieval and Preprocessing</h2>\n",
    "    <ul>\n",
    "        <li>Iterate through the dataset or data source to retrieve URLs.</li>\n",
    "        <li>Attempt to fetch text content from each URL using the <code>extract_text_from_url()</code> function.</li>\n",
    "        <li>If fetching fails, print an error message and drop the corresponding row from the DataFrame.</li>\n",
    "    </ul>\n",
    "\n",
    "    <h2>3. Text Processing and Metric Calculation</h2>\n",
    "    <ul>\n",
    "        <li>If text content is successfully fetched, process it using <code>text_process()</code> function.</li>\n",
    "        <li>Tokenize the processed text into words.</li>\n",
    "        <li>Calculate various metrics based on the text content, including positive and negative scores, polarity and subjectivity scores, average sentence length, percentage of complex words, Fog index, etc.</li>\n",
    "    </ul>\n",
    "\n",
    "    <h2>4. Save the ouput into .csv file<h2>\n",
    "</body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a84ea1f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/prachi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "304d1004",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing input file\n",
    "df = pd.read_excel('Input.xlsx', usecols=['URL_ID', 'URL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2708ffe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d232ada0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blackassign0001</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blackassign0002</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blackassign0003</td>\n",
       "      <td>https://insights.blackcoffer.com/internet-dema...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blackassign0004</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-cyber...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blackassign0005</td>\n",
       "      <td>https://insights.blackcoffer.com/ott-platform-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>blackassign0006</td>\n",
       "      <td>https://insights.blackcoffer.com/the-rise-of-t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>blackassign0007</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-cyber...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>blackassign0008</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-inter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>blackassign0009</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-cyber...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>blackassign0010</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-cyber...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            URL_ID                                                URL\n",
       "0  blackassign0001  https://insights.blackcoffer.com/rising-it-cit...\n",
       "1  blackassign0002  https://insights.blackcoffer.com/rising-it-cit...\n",
       "2  blackassign0003  https://insights.blackcoffer.com/internet-dema...\n",
       "3  blackassign0004  https://insights.blackcoffer.com/rise-of-cyber...\n",
       "4  blackassign0005  https://insights.blackcoffer.com/ott-platform-...\n",
       "5  blackassign0006  https://insights.blackcoffer.com/the-rise-of-t...\n",
       "6  blackassign0007  https://insights.blackcoffer.com/rise-of-cyber...\n",
       "7  blackassign0008  https://insights.blackcoffer.com/rise-of-inter...\n",
       "8  blackassign0009  https://insights.blackcoffer.com/rise-of-cyber...\n",
       "9  blackassign0010  https://insights.blackcoffer.com/rise-of-cyber..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06737cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_url(url, retries=3, backoff_factor=0.3):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36'}\n",
    "    \n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            page = requests.get(url, headers=headers)\n",
    "            page.raise_for_status()  # Raises an HTTPError for bad responses\n",
    "            soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "            # Extract the content\n",
    "            content_tags = soup.find_all(attrs={'class': 'td-post-content'})\n",
    "            content = ' '.join([tag.get_text(strip=True) for tag in content_tags])\n",
    "\n",
    "            # Clean the text\n",
    "            content = content.replace('\\xa0', ' ').replace('\\n', ' ')\n",
    "\n",
    "            return content\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Attempt {attempt + 1} failed: {e}\")\n",
    "            time.sleep(backoff_factor * (2 ** attempt))  # Exponential backoff\n",
    "            \n",
    "    print(f\"Failed to fetch {url} after {retries} attempts\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f3426e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom function for text processing\n",
    "def text_process(text):\n",
    "    nopunc = [char for char in text if char not in string.punctuation]\n",
    "    nopunc = ''.join(nopunc)\n",
    "    return ' '.join([word.lower() for word in nopunc.split() if word.lower() not in stopwords])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f7d1e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load stopwords and dictionaries\n",
    "stopwords_files = [\n",
    "    \"StopWords/StopWords_Auditor.txt\",\n",
    "    \"StopWords/StopWords_Currencies.txt\",\n",
    "    \"StopWords/StopWords_DatesandNumbers.txt\",\n",
    "    \"StopWords/StopWords_Generic.txt\",\n",
    "    \"StopWords/StopWords_GenericLong.txt\",\n",
    "    \"StopWords/StopWords_Geographic.txt\",\n",
    "    \"StopWords/StopWords_Names.txt\"\n",
    "]\n",
    "\n",
    "stopwords = set()\n",
    "for file in stopwords_files:\n",
    "    with open(file, 'r', encoding='ISO-8859-1') as f:\n",
    "        stopwords.update(f.read().split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1de84923",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_words = set(open(\"MasterDictionary/positive-words.txt\", 'r').read().split())\n",
    "negative_words = set(open(\"MasterDictionary/negative-words.txt\", 'r', encoding=\"ISO-8859-1\").read().split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c8322b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to count syllables\n",
    "def syllable_count(word):\n",
    "    vowels = 'aeiou'\n",
    "    word = word.lower()\n",
    "    count = 0\n",
    "    if word[0] in vowels:\n",
    "        count += 1\n",
    "    for index in range(1, len(word)):\n",
    "        if word[index] in vowels and word[index - 1] not in vowels:\n",
    "            count += 1\n",
    "    if word.endswith(\"es\") or word.endswith(\"ed\"):\n",
    "        count -= 1\n",
    "    return max(1, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5493fb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store scores for each URL\n",
    "positive_scores = []\n",
    "negative_scores = []\n",
    "polarity_scores = []\n",
    "subjectivity_scores = []\n",
    "avg_sentence_lengths = []\n",
    "percentage_of_complex_wo = []\n",
    "fog_indices = []\n",
    "avg_words_per_sent = []\n",
    "complex_word_counts = []\n",
    "word_counts = []\n",
    "syllable_counts = []\n",
    "personal_pronouns = []\n",
    "avg_word_lengths = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "83208fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Personal pronouns list\n",
    "personal_pronouns_list = ['i', 'we', 'my', 'ours', 'us']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1ee9a4ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url_id:  1\n",
      "url_id:  2\n",
      "url_id:  3\n",
      "url_id:  4\n",
      "url_id:  5\n",
      "url_id:  6\n",
      "url_id:  7\n",
      "url_id:  8\n",
      "url_id:  9\n",
      "url_id:  10\n",
      "url_id:  11\n",
      "url_id:  12\n",
      "url_id:  13\n",
      "url_id:  14\n",
      "url_id:  15\n",
      "url_id:  16\n",
      "url_id:  17\n",
      "url_id:  18\n",
      "url_id:  19\n",
      "url_id:  20\n",
      "url_id:  21\n",
      "url_id:  22\n",
      "url_id:  23\n",
      "url_id:  24\n",
      "url_id:  25\n",
      "url_id:  26\n",
      "url_id:  27\n",
      "url_id:  28\n",
      "url_id:  29\n",
      "url_id:  30\n",
      "url_id:  31\n",
      "url_id:  32\n",
      "url_id:  33\n",
      "url_id:  34\n",
      "url_id:  35\n",
      "url_id:  36\n",
      "Attempt 1 failed: 404 Client Error: Not Found for url: https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/\n",
      "Deleted the url. https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/\n",
      "url_id:  37\n",
      "url_id:  38\n",
      "url_id:  39\n",
      "url_id:  40\n",
      "url_id:  41\n",
      "url_id:  42\n",
      "url_id:  43\n",
      "url_id:  44\n",
      "url_id:  45\n",
      "url_id:  46\n",
      "url_id:  47\n",
      "url_id:  48\n",
      "url_id:  49\n",
      "Attempt 1 failed: 404 Client Error: Not Found for url: https://insights.blackcoffer.com/covid-19-environmental-impact-for-the-future/\n",
      "Deleted the url. https://insights.blackcoffer.com/covid-19-environmental-impact-for-the-future/\n",
      "url_id:  50\n",
      "url_id:  51\n",
      "url_id:  52\n",
      "url_id:  53\n",
      "url_id:  54\n",
      "url_id:  55\n",
      "url_id:  56\n",
      "url_id:  57\n",
      "url_id:  58\n",
      "url_id:  59\n",
      "url_id:  60\n",
      "url_id:  61\n",
      "url_id:  62\n",
      "url_id:  63\n",
      "url_id:  64\n",
      "url_id:  65\n",
      "url_id:  66\n",
      "url_id:  67\n",
      "url_id:  68\n",
      "url_id:  69\n",
      "url_id:  70\n",
      "url_id:  71\n",
      "url_id:  72\n",
      "url_id:  73\n",
      "url_id:  74\n",
      "url_id:  75\n",
      "url_id:  76\n",
      "url_id:  77\n",
      "url_id:  78\n",
      "url_id:  79\n",
      "url_id:  80\n",
      "url_id:  81\n",
      "url_id:  82\n",
      "url_id:  83\n",
      "url_id:  84\n",
      "url_id:  85\n",
      "url_id:  86\n",
      "url_id:  87\n",
      "url_id:  88\n",
      "url_id:  89\n",
      "url_id:  90\n",
      "url_id:  91\n",
      "url_id:  92\n",
      "url_id:  93\n",
      "url_id:  94\n",
      "url_id:  95\n",
      "url_id:  96\n",
      "url_id:  97\n",
      "url_id:  98\n",
      "url_id:  99\n",
      "url_id:  100\n"
     ]
    }
   ],
   "source": [
    "# Iterate over each URL\n",
    "cnt = 1\n",
    "for i, row in df.iterrows():\n",
    "    print(\"url_id: \", cnt)\n",
    "    url = row['URL']\n",
    "    try:\n",
    "        text = extract_text_from_url(url)\n",
    "        if text:\n",
    "            # Process the extracted text\n",
    "            processed_text = text_process(text)\n",
    "            \n",
    "            # Tokenize the processed text\n",
    "            tokens = word_tokenize(processed_text)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            positive_score_url = sum(1 for word in tokens if word in positive_words)\n",
    "            negative_score_url = sum(1 for word in tokens if word in negative_words)\n",
    "            polarity_score_url = (positive_score_url - negative_score_url) / (positive_score_url + negative_score_url + 1e-6)\n",
    "            subjectivity_score_url = (positive_score_url + negative_score_url) / (len(tokens) + 1e-6)\n",
    "            \n",
    "            sentences = processed_text.split('.')\n",
    "            avg_sentence_length = len(tokens) / len(sentences)\n",
    "            \n",
    "            complex_words = [word for word in tokens if len(word) > 2 and syllable_count(word) > 2]\n",
    "            percentage_of_complex_words = len(complex_words) / len(tokens)\n",
    "            \n",
    "            fog_index = 0.4 * (avg_sentence_length + percentage_of_complex_words)\n",
    "            \n",
    "            avg_words_per_sentence = len(tokens) / len(sentences)\n",
    "            \n",
    "            complex_word_count_url = len(complex_words)\n",
    "            word_count = len(tokens)\n",
    "            \n",
    "            syllable_count_total = sum(syllable_count(word) for word in tokens)\n",
    "            \n",
    "            personal_pronoun_count = sum(1 for word in tokens if word in personal_pronouns_list)\n",
    "            \n",
    "            avg_word_length = sum(len(word) for word in tokens) / len(tokens)\n",
    "            \n",
    "            # Append scores to respective lists\n",
    "            positive_scores.append(positive_score_url)\n",
    "            negative_scores.append(negative_score_url)\n",
    "            polarity_scores.append(polarity_score_url)\n",
    "            subjectivity_scores.append(subjectivity_score_url)\n",
    "            avg_sentence_lengths.append(avg_sentence_length)\n",
    "            percentage_of_complex_wo.append(percentage_of_complex_words)\n",
    "            fog_indices.append(fog_index)\n",
    "            avg_words_per_sent.append(avg_words_per_sentence)\n",
    "            complex_word_counts.append(complex_word_count_url)\n",
    "            word_counts.append(word_count)\n",
    "            syllable_counts.append(syllable_count_total)\n",
    "            personal_pronouns.append(personal_pronoun_count)\n",
    "            avg_word_lengths.append(avg_word_length)\n",
    "    except Exception:\n",
    "        \n",
    "        print(\"Deleted the url.\",url)\n",
    "        # Drop the row where URL matches the problematic URL\n",
    "        df.drop(df.index[df['URL'] == url], inplace=True)\n",
    "    cnt+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f6db5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame to store the scores\n",
    "scores_df = pd.DataFrame({\n",
    "    'URL_ID':df['URL_ID'],\n",
    "    'URL': df['URL'],\n",
    "    'Positive Score': positive_scores,\n",
    "    'Negative Score': negative_scores,\n",
    "    'Polarity Score': polarity_scores,\n",
    "    'Subjectivity Score': subjectivity_scores,\n",
    "    'Average Sentence Length': avg_sentence_lengths,\n",
    "    'Percentage of Complex Words': percentage_of_complex_words,\n",
    "    'Fog Index': fog_indices,\n",
    "    'Average Words per Sentence': avg_words_per_sentence,\n",
    "    'Complex Word Count': complex_word_counts,\n",
    "    'Word Count': word_counts,\n",
    "    'Syllable Count': syllable_counts,\n",
    "    'Personal Pronouns': personal_pronouns,\n",
    "    'Average Word Length': avg_word_lengths\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0183b6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the DataFrame\n",
    "print(scores_df)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "scores_df.to_csv('output_scores.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2f66f7",
   "metadata": {},
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
